{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU_full_grad(torch.autograd.Function):\n",
    "    \"\"\" ReLU activation function that passes through the gradient irrespective of its input value. \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(input):\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(grad_output):\n",
    "        return grad_output.clone()\n",
    "\n",
    "class FwdFwdModel(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(FwdFwdModel, self).__init__()\n",
    "\n",
    "        self.act_fn = nn.ReLU()\n",
    "\n",
    "        self.model = nn.ModuleList([])\n",
    "        self.optimizers = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.model.append(nn.Linear(layers[i], layers[i+1]))\n",
    "            self.optimizers.append(torch.optim.Adam(self.model[len(self.model) - 1].parameters()))\n",
    "\n",
    "        self.ff_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        self.linear_classifer = nn.Sequential(\n",
    "            # TODO (should we omit the first Linear from classifier?)\n",
    "            nn.Linear(sum(layers[1:]), 10, bias=False) # 10 = the number of classes\n",
    "        )\n",
    "        self.classification_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # TODO (understand this param init fn)\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight, mean=0, std=1 / math.sqrt(m.weight.shape[0]))\n",
    "            # torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "        for m in self.linear_classifer.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.zeros_(m.weight)\n",
    "\n",
    "    def _calc_ff_loss(self, z, labels):\n",
    "        sum_of_squares = torch.sum(z ** 2, dim=-1)\n",
    "        logits = sum_of_squares - z.shape[0] # TODO (not sure if authors used size of layer as threshold)\n",
    "        logits = torch.sigmoid(logits)\n",
    "        ff_loss = self.ff_loss(logits, labels)\n",
    "        \n",
    "        return ff_loss\n",
    "    \n",
    "    def _layer_norm(self, z, eps=1e-8):\n",
    "        return z / (torch.sqrt(torch.mean(z ** 2, dim=-1, keepdim=True)) + eps)\n",
    "    \n",
    "    def _linear_classifier_fwd(self, input, label):\n",
    "        # append all weight tensors into single tensor\n",
    "        neural_sample = input[0]\n",
    "        for i in range(1, len(input)):\n",
    "            # TODO (line below might break when batch size > 1)\n",
    "            neural_sample = torch.cat((neural_sample, input[i]), 0)\n",
    "\n",
    "        # forward pass through classifier\n",
    "        output = self.linear_classifer(neural_sample.detach())\n",
    "        output = output - torch.max(output, dim=-1, keepdim=True)[0] # TODO (not entirely clear why each entry in output is made 0 or -ve)\n",
    "\n",
    "        # loss\n",
    "        classification_loss = self.classification_loss(output, label)\n",
    "\n",
    "        # return \n",
    "        return classification_loss, output\n",
    "    \n",
    "    def forward(self, inputs, ff_labels, class_labels):\n",
    "        # scalar_outputs = {\n",
    "        #     \"Loss\": torch.zeros(1, device=\"cpu\")\n",
    "        # }\n",
    "\n",
    "        # breakpoint()\n",
    "\n",
    "        z = inputs\n",
    "\n",
    "        optim_idx = 0\n",
    "        neural_sample = []\n",
    "        for idx, layer in enumerate(self.model):\n",
    "            z = z.detach()  # Detach to ensure no computation graph reuse\n",
    "            z.requires_grad_()\n",
    "            z = layer(z)\n",
    "            z = self.act_fn(z) # forward through layer\n",
    "            neural_sample.append(z)\n",
    "            ff_loss = self._calc_ff_loss(z, ff_labels) # calc layer wise loss\n",
    "            self.optimizers[optim_idx].zero_grad()\n",
    "\n",
    "            ff_loss.backward(retain_graph=True) # compute gradients for layer\n",
    "\n",
    "            self.optimizers[optim_idx].step() # step forward\n",
    "            optim_idx += 1\n",
    "\n",
    "            z = self._layer_norm(z) # normalize for next layer\n",
    "\n",
    "\n",
    "        #scalar_outputs[f\"loss_layer_{idx}\"] = ff_loss\n",
    "        if ff_labels:\n",
    "            return self._linear_classifier_fwd(neural_sample, class_labels)\n",
    "        return None, None\n",
    "    \n",
    "    def get_linear_classifier_param(self):\n",
    "        return self.linear_classifer.parameters()\n",
    "    \n",
    "    def infer(self, inputs):\n",
    "        z = inputs\n",
    "        neural_sample = []\n",
    "        with torch.no_grad():\n",
    "            for idx, layer in enumerate(self.model):\n",
    "                z = layer(z)\n",
    "                z = self.act_fn(z)\n",
    "                z = self._layer_norm(z)\n",
    "                neural_sample.append(z)\n",
    "            lr_input = neural_sample[0]\n",
    "            for i in range(1, len(neural_sample)):\n",
    "                lr_input = torch.cat((lr_input, neural_sample[i]), 0)\n",
    "            output = self.linear_classifer(lr_input)\n",
    "            return output - torch.max(output, dim=-1, keepdim=True)[0]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the images to [-1, 1]\n",
    "])\n",
    "\n",
    "print(\"Downloading the testing dataset to ./data\")\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,  # Load the test set\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False  # No need to shuffle the test set\n",
    ")\n",
    "\n",
    "# Example: Iterate through the training DataLoader\n",
    "for image, label in test_loader:\n",
    "    image = image.squeeze(0)  # Remove the batch dimension, making it [1, 28, 28]\n",
    "    label = label.item()  # Convert the label tensor to a Python integer\n",
    "    print(f\"Image shape: {image.shape}, Label: {label}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sample(inputs): \n",
    "    # TODO (this will break when batch size is greater than 1)\n",
    "    one_hot_label = torch.zeros([10])\n",
    "    # concatinate the label as a 1 hot encoding and flattened image\n",
    "    return torch.cat((one_hot_label, torch.flatten(inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample):\n",
    "    image = sample.squeeze(0)\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.title(\"MNIST Sample\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = 0\n",
    "i = 0\n",
    "total = len(test_loader.dataset)\n",
    "for inputs, labels, in test_loader:\n",
    "    i += 1\n",
    "    truth = labels.item()\n",
    "    prediction = model.infer(preprocess_sample(inputs)).argmax() + 1\n",
    "    if truth != prediction:\n",
    "        #print_sample(inputs[0])\n",
    "        wrong += 1\n",
    "    print(f\"Label: {truth} Pred: {prediction} Error rate: {wrong} / {i} ({round(wrong/i*100, 2)}%)\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "\n",
    "layers = [794, 2000, 2000, 2000]\n",
    "untrained_model = FwdFwdModel(layers)\n",
    "\n",
    "wrong = 0\n",
    "i = 0\n",
    "total = len(test_loader.dataset)\n",
    "for inputs, labels, in test_loader:\n",
    "    i += 1\n",
    "    truth = labels.item()\n",
    "    prediction = untrained_model.infer(preprocess_sample(inputs)).argmax() + 1\n",
    "    if truth != prediction:\n",
    "        wrong += 1\n",
    "    print(f\"Label: {truth} Pred: {prediction} Error rate: {wrong} / {i} ({round(wrong/i*100, 2)}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
